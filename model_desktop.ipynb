{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c72329ca",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5030cf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (repodata.json): ...working... done\n",
      "Solving environment: ...working... failed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "PackagesNotFoundError: The following packages are missing from the target environment:\n",
      "  - mediapipe\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!pip install opencv-python mediapipe numpy pandas scikit-learn joblib tqdm matplotlib seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a1fff133",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "DLL load failed while importing _framework_bindings: A dynamic link library (DLL) initialization routine failed.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Import libraries for video processing, pose estimation, ML, and visualization\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcv2\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmediapipe\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mmp\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\mediapipe\\__init__.py:15\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Copyright 2019 - 2022 The MediaPipe Authors.\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# See the License for the specific language governing permissions and\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmediapipe\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmediapipe\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msolutions\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01msolutions\u001b[39;00m \n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmediapipe\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtasks\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtasks\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\mediapipe\\python\\__init__.py:17\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Copyright 2020-2021 The MediaPipe Authors.\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# See the License for the specific language governing permissions and\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;124;03m\"\"\"MediaPipe Python API.\"\"\"\u001b[39;00m\n\u001b[1;32m---> 17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmediapipe\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_framework_bindings\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m model_ckpt_util\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmediapipe\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_framework_bindings\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m resource_util\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmediapipe\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_framework_bindings\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcalculator_graph\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CalculatorGraph\n",
      "\u001b[1;31mImportError\u001b[0m: DLL load failed while importing _framework_bindings: A dynamic link library (DLL) initialization routine failed."
     ]
    }
   ],
   "source": [
    "# Import libraries for video processing, pose estimation, ML, and visualization\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import joblib\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Initialize MediaPipe Pose and Drawing utilities\n",
    "mp_pose = mp.solutions.pose\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "pose = mp_pose.Pose(static_image_mode=False, min_detection_confidence=0.5, min_tracking_confidence=0.5)\n",
    "\n",
    "# Define dataset paths and constants\n",
    "DATASET_DIR = \"dataset_small\"\n",
    "SUBFOLDERS = [\"train\", \"val\", \"test\"]\n",
    "CLASSES = [\"Arm Raise Correct\", \"Arm Raise Incorrect\"]\n",
    "LABEL_MAP = {\"Arm Raise Correct\": 1, \"Arm Raise Incorrect\": 0}\n",
    "\n",
    "print(\"Libraries imported and MediaPipe initialized.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d95651c0",
   "metadata": {},
   "source": [
    "# Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16e8c3ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract landmarks from a frame\n",
    "def extract_landmarks(frame):\n",
    "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    results = pose.process(frame_rgb)\n",
    "    if results.pose_landmarks:\n",
    "        landmarks = results.pose_landmarks.landmark\n",
    "        # Extract x, y, z, visibility for all 33 keypoints\n",
    "        features = []\n",
    "        for lm in landmarks:\n",
    "            features.extend([lm.x, lm.y, lm.z, lm.visibility])\n",
    "        return np.array(features), results.pose_landmarks\n",
    "    return None, None\n",
    "\n",
    "# Function to compute joint angles (e.g., elbow angle)\n",
    "def compute_angle(p1, p2, p3):\n",
    "    # p1, p2, p3 are landmark coordinates (x, y, z)\n",
    "    a = np.array([p1.x, p1.y, p1.z])\n",
    "    b = np.array([p2.x, p2.y, p2.z])\n",
    "    c = np.array([p3.x, p3.y, p3.z])\n",
    "    ba = a - b\n",
    "    bc = c - b\n",
    "    cosine_angle = np.dot(ba, bc) / (np.linalg.norm(ba) * np.linalg.norm(bc))\n",
    "    angle = np.arccos(np.clip(cosine_angle, -1.0, 1.0))\n",
    "    return np.degrees(angle)\n",
    "\n",
    "# Function to compute distance between two points\n",
    "def compute_distance(p1, p2):\n",
    "    return np.sqrt((p1.x - p2.x)**2 + (p1.y - p2.y)**2 + (p1.z - p2.z)**2)\n",
    "\n",
    "# Function to normalize landmarks relative to torso and scale\n",
    "def normalize_landmarks(features, landmarks):\n",
    "    # Normalize by subtracting torso midpoint (average of shoulders)\n",
    "    shoulder_left = landmarks[mp_pose.PoseLandmark.LEFT_SHOULDER]\n",
    "    shoulder_right = landmarks[mp_pose.PoseLandmark.RIGHT_SHOULDER]\n",
    "    torso_x = (shoulder_left.x + shoulder_right.x) / 2\n",
    "    torso_y = (shoulder_left.y + shoulder_right.y) / 2\n",
    "    # Compute shoulder width as scaling factor\n",
    "    shoulder_width = compute_distance(shoulder_left, shoulder_right)\n",
    "    features_normalized = []\n",
    "    for i in range(0, len(features), 4):\n",
    "        features_normalized.extend([\n",
    "            (features[i] - torso_x) / shoulder_width,  # x\n",
    "            (features[i + 1] - torso_y) / shoulder_width,  # y\n",
    "            features[i + 2] / shoulder_width,  # z\n",
    "            features[i + 3]  # visibility\n",
    "        ])\n",
    "    return np.array(features_normalized)\n",
    "\n",
    "# Function to extract features (landmarks + angles + distances) from a frame\n",
    "def extract_features(frame):\n",
    "    landmarks_array, landmarks = extract_landmarks(frame)\n",
    "    if landmarks_array is None:\n",
    "        return None\n",
    "    # Normalize landmarks\n",
    "    features = normalize_landmarks(landmarks_array, landmarks)\n",
    "    # Compute additional features: elbow angles and shoulder-to-wrist distances\n",
    "    left_elbow_angle = compute_angle(\n",
    "        landmarks[mp_pose.PoseLandmark.LEFT_SHOULDER],\n",
    "        landmarks[mp_pose.PoseLandmark.LEFT_ELBOW],\n",
    "        landmarks[mp_pose.PoseLandmark.LEFT_WRIST]\n",
    "    )\n",
    "    right_elbow_angle = compute_angle(\n",
    "        landmarks[mp_pose.PoseLandmark.RIGHT_SHOULDER],\n",
    "        landmarks[mp_pose.PoseLandmark.RIGHT_ELBOW],\n",
    "        landmarks[mp_pose.PoseLandmark.RIGHT_WRIST]\n",
    "    )\n",
    "    left_shoulder_wrist_dist = compute_distance(\n",
    "        landmarks[mp_pose.PoseLandmark.LEFT_SHOULDER],\n",
    "        landmarks[mp_pose.PoseLandmark.LEFT_WRIST]\n",
    "    ) / compute_distance(landmarks[mp_pose.PoseLandmark.LEFT_SHOULDER], landmarks[mp_pose.PoseLandmark.RIGHT_SHOULDER])\n",
    "    right_shoulder_wrist_dist = compute_distance(\n",
    "        landmarks[mp_pose.PoseLandmark.RIGHT_SHOULDER],\n",
    "        landmarks[mp_pose.PoseLandmark.RIGHT_WRIST]\n",
    "    ) / compute_distance(landmarks[mp_pose.PoseLandmark.LEFT_SHOULDER], landmarks[mp_pose.PoseLandmark.RIGHT_SHOULDER])\n",
    "    # Combine features\n",
    "    features = np.append(features, [left_elbow_angle, right_elbow_angle, left_shoulder_wrist_dist, right_shoulder_wrist_dist])\n",
    "    return features\n",
    "\n",
    "# Function to process a video and extract averaged features\n",
    "def process_video(video_path, frame_skip=5):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    features_list = []\n",
    "    frame_count = 0\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        if frame_count % frame_skip == 0:\n",
    "            features = extract_features(frame)\n",
    "            if features is not None:\n",
    "                features_list.append(features)\n",
    "        frame_count += 1\n",
    "    cap.release()\n",
    "    if features_list:\n",
    "        return np.mean(features_list, axis=0)  # Average features across frames\n",
    "    return None\n",
    "\n",
    "# Function to plot confusion matrix\n",
    "def plot_confusion_matrix(y_true, y_pred, title):\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Incorrect', 'Correct'], yticklabels=['Incorrect', 'Correct'])\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.show()\n",
    "\n",
    "print(\"Helper functions defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70b96189",
   "metadata": {},
   "source": [
    "# Preprocess Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56d6da16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to preprocess dataset for a given frame skip\n",
    "def preprocess_dataset(frame_skip):\n",
    "    X_train, y_train = [], []\n",
    "    X_val, y_val = [], []\n",
    "    X_test, y_test = [], []\n",
    "    \n",
    "    for split in SUBFOLDERS:\n",
    "        for class_name in CLASSES:\n",
    "            folder_path = os.path.join(DATASET_DIR, split, class_name)\n",
    "            videos = [os.path.join(folder_path, f) for f in os.listdir(folder_path) if f.endswith(\".mp4\")]\n",
    "            label = LABEL_MAP[class_name]\n",
    "            for video in tqdm(videos, desc=f\"Processing {split}/{class_name}\"):\n",
    "                features = process_video(video, frame_skip=frame_skip)\n",
    "                if features is not None:\n",
    "                    if split == \"train\":\n",
    "                        X_train.append(features)\n",
    "                        y_train.append(label)\n",
    "                    elif split == \"val\":\n",
    "                        X_val.append(features)\n",
    "                        y_val.append(label)\n",
    "                    else:  # test\n",
    "                        X_test.append(features)\n",
    "                        y_test.append(label)\n",
    "    \n",
    "    return (np.array(X_train), np.array(y_train),\n",
    "            np.array(X_val), np.array(y_val),\n",
    "            np.array(X_test), np.array(y_test))\n",
    "\n",
    "# Test different frame skip rates\n",
    "frame_skips = [1, 3, 5, 10]\n",
    "val_accuracies = []\n",
    "best_frame_skip = 5  # Default\n",
    "best_val_accuracy = 0\n",
    "best_data = None\n",
    "\n",
    "for frame_skip in frame_skips:\n",
    "    print(f\"\\nTesting frame skip: {frame_skip}\")\n",
    "    data = preprocess_dataset(frame_skip)\n",
    "    X_train, y_train, X_val, y_val, X_test, y_test = data\n",
    "    # Train a quick model to evaluate\n",
    "    model = RandomForestClassifier(n_estimators=50, random_state=42)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_val_pred = model.predict(X_val)\n",
    "    val_accuracy = accuracy_score(y_val, y_val_pred)\n",
    "    val_accuracies.append(val_accuracy)\n",
    "    print(f\"Validation Accuracy: {val_accuracy:.4f}\")\n",
    "    if val_accuracy > best_val_accuracy:\n",
    "        best_val_accuracy = val_accuracy\n",
    "        best_frame_skip = frame_skip\n",
    "        best_data = data\n",
    "\n",
    "# Select best frame skip\n",
    "X_train, y_train, X_val, y_val, X_test, y_test = best_data\n",
    "print(f\"\\nSelected frame skip: {best_frame_skip} with validation accuracy: {best_val_accuracy:.4f}\")\n",
    "print(f\"Data shapes: X_train: {X_train.shape}, X_val: {X_val.shape}, X_test: {X_test.shape}\")\n",
    "\n",
    "# Save preprocessed data\n",
    "np.savez(\"preprocessed_data.npz\", X_train=X_train, y_train=y_train, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4ad7b12",
   "metadata": {},
   "source": [
    "# Training Random Forest Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9b3c4ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test feature sets: raw landmarks, normalized landmarks, full features\n",
    "feature_sets = {\n",
    "    \"raw\": X_train[:, :132],  # Only landmarks\n",
    "    \"normalized\": X_train,  # Normalized landmarks + angles + distances\n",
    "}\n",
    "best_model = None\n",
    "best_features = None\n",
    "best_val_accuracy = 0\n",
    "\n",
    "for name, X_train_subset in feature_sets.items():\n",
    "    print(f\"\\nTraining with {name} features\")\n",
    "    # Prepare corresponding validation set\n",
    "    if name == \"raw\":\n",
    "        X_val_subset = X_val[:, :132]\n",
    "    else:\n",
    "        X_val_subset = X_val\n",
    "    # Define parameter grid\n",
    "    param_grid = {\n",
    "        'n_estimators': [50, 100, 200],\n",
    "        'max_depth': [5, 10, None]\n",
    "    }\n",
    "    model = RandomForestClassifier(random_state=42)\n",
    "    grid_search = GridSearchCV(model, param_grid, cv=3, scoring='accuracy')\n",
    "    grid_search.fit(X_train_subset, y_train)\n",
    "    # Evaluate on validation set\n",
    "    y_val_pred = grid_search.predict(X_val_subset)\n",
    "    val_accuracy = accuracy_score(y_val, y_val_pred)\n",
    "    print(f\"Validation Accuracy with {name} features: {val_accuracy:.4f}\")\n",
    "    if val_accuracy > best_val_accuracy:\n",
    "        best_val_accuracy = val_accuracy\n",
    "        best_model = grid_search.best_estimator_\n",
    "        best_features = name\n",
    "        best_X_train = X_train_subset\n",
    "        best_X_val = X_val_subset\n",
    "        best_X_test = X_test[:, :132] if name == \"raw\" else X_test\n",
    "\n",
    "print(f\"\\nBest model uses {best_features} features with validation accuracy: {best_val_accuracy:.4f}\")\n",
    "print(f\"Best parameters: {best_model.get_params()}\")\n",
    "\n",
    "# Save best model\n",
    "joblib.dump(best_model, \"postureguardian_model.joblib\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51ca6aa0",
   "metadata": {},
   "source": [
    "# Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0482f688",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model on test set\n",
    "y_test_pred = best_model.predict(best_X_test)\n",
    "accuracy = accuracy_score(y_test, y_test_pred)\n",
    "precision = precision_score(y_test, y_test_pred)\n",
    "recall = recall_score(y_test, y_test_pred)\n",
    "f1 = f1_score(y_test, y_test_pred)\n",
    "\n",
    "print(\"Test Set Evaluation:\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1-Score: {f1:.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_test_pred, target_names=['Incorrect', 'Correct']))\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(6, 4))\n",
    "cm = confusion_matrix(y_test, y_test_pred)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Incorrect', 'Correct'], yticklabels=['Incorrect', 'Correct'])\n",
    "plt.title('Confusion Matrix - Test Set')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.show()\n",
    "\n",
    "# Plot precision, recall, F1-score\n",
    "metrics = ['Accuracy', 'Recall', 'Precision', 'F1-Score']\n",
    "values = [accuracy, recall, precision, f1]\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.barplot(x=metrics, y=values, palette='viridis')\n",
    "plt.title('Model Performance Metrics')\n",
    "plt.ylabel('Score')\n",
    "plt.ylim(0, 1)\n",
    "for i, v in enumerate(values):\n",
    "    plt.text(i, v + 0.02, f'{v:.4f}', ha='center')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56d666f8",
   "metadata": {},
   "source": [
    "# Real-Time Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "292593f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to draw arm-focused skeleton\n",
    "def draw_arm_skeleton(frame, landmarks):\n",
    "    arm_connections = [\n",
    "        (mp_pose.PoseLandmark.LEFT_SHOULDER, mp_pose.PoseLandmark.LEFT_ELBOW),\n",
    "        (mp_pose.PoseLandmark.LEFT_ELBOW, mp_pose.PoseLandmark.LEFT_WRIST),\n",
    "        (mp_pose.PoseLandmark.RIGHT_SHOULDER, mp_pose.PoseLandmark.RIGHT_ELBOW),\n",
    "        (mp_pose.PoseLandmark.RIGHT_ELBOW, mp_pose.PoseLandmark.RIGHT_WRIST)\n",
    "    ]\n",
    "    for connection in arm_connections:\n",
    "        start = landmarks.landmark[connection[0]]\n",
    "        end = landmarks.landmark[connection[1]]\n",
    "        cv2.line(frame, \n",
    "                 (int(start.x * frame.shape[1]), int(start.y * frame.shape[0])),\n",
    "                 (int(end.x * frame.shape[1]), int(end.y * frame.shape[0])),\n",
    "                 (255, 255, 255), 2)\n",
    "        cv2.circle(frame, \n",
    "                   (int(start.x * frame.shape[1]), int(start.y * frame.shape[0])), \n",
    "                   5, (255, 0, 0), -1)\n",
    "        cv2.circle(frame, \n",
    "                   (int(end.x * frame.shape[1]), int(end.y * frame.shape[0])), \n",
    "                   5, (255, 0, 0), -1)\n",
    "\n",
    "# Real-time inference\n",
    "def real_time_inference(model, feature_type='normalized', window_size=10):\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    predictions = []\n",
    "    \n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        # Extract features\n",
    "        features = extract_features(frame)\n",
    "        if features is not None:\n",
    "            # Select features based on best model\n",
    "            if feature_type == 'raw':\n",
    "                features = features[:132]\n",
    "            # Predict\n",
    "            prediction = model.predict([features])[0]\n",
    "            predictions.append(prediction)\n",
    "            # Average predictions over window\n",
    "            if len(predictions) > window_size:\n",
    "                predictions.pop(0)\n",
    "            avg_prediction = np.mean(predictions) > 0.5\n",
    "            label = \"Correct\" if avg_prediction else \"Incorrect\"\n",
    "            color = (0, 255, 0) if avg_prediction else (0, 0, 255)\n",
    "            # Display label\n",
    "            cv2.putText(frame, label, (50, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, color, 2)\n",
    "            # Draw arm skeleton\n",
    "            _, landmarks = extract_landmarks(frame)\n",
    "            if landmarks:\n",
    "                draw_arm_skeleton(frame, landmarks)\n",
    "        cv2.imshow(\"PostureGuardian\", frame)\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "# Run real-time inference\n",
    "print(\"Starting real-time inference (press 'q' to quit)...\")\n",
    "real_time_inference(best_model, feature_type=best_features)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
